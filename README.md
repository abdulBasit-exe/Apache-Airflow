# Apache Airflow #
- Apache Airflow is a Workflow Orchestration tool designed by Apache to make job of building of Data Pipelines easy and manageable.

- Airflow is one of the most used tool for Data Engineers which enables them to write, manage and automate Data Pipelines. Airflow makes visualization of Pipelines much more easier. It provides a Command Line Interface as well as a GUI through web server to interact with the workflows and monitor them efficiently.

- In Airflow a workflow is represented in the form of a DAG(Directed Acyclic Graphs). It can be easily written in Python. It enables you to write efficient etl jobs and schedule them easily. A DAG ensures that multiple workflows run in a defined order, handle errors if happening, and retry upon failure.

### How to build and automate a workflow using Airflow?

### Create a DAG
  A DAG generally is based upon these five elements: 
.   Importing necessary libraries
.   Defining the default arguments 
.   Instantiating DAG
.   Defining tasks
.   Task Dependency

### How to use Airflow?

Install Apache Airflow Using pip

Create a virtual Environment and activate it.

Initiliase database 

Airflow webserver and scheduler command
